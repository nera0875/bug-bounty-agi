# Guide Migration Base de Donn√©es - Syst√®me IA

## üöÄ √âtapes pour appliquer les migrations

### 1. Connexion au Dashboard Supabase
1. Ouvrir https://supabase.com/dashboard
2. S√©lectionner votre projet
3. Aller dans **SQL Editor** (ic√¥ne SQL dans la sidebar)

### 2. Application des migrations

#### Option A: Application compl√®te (recommand√©e)
1. Cliquer sur **New Query**
2. Copier tout le contenu SQL g√©n√©r√© par `node scripts/apply-migrations.js` option 2
3. Coller dans l'√©diteur SQL
4. Cliquer sur **Run** (ou Ctrl+Enter)

#### Option B: Application progressive (si erreurs)
Si certaines tables existent d√©j√†, appliquer par section:

1. **Extensions requises** (si pas d√©j√† install√©):
```sql
CREATE EXTENSION IF NOT EXISTS vector;
CREATE EXTENSION IF NOT EXISTS pg_trgm;
```

2. **Modifications table projects**:
```sql
ALTER TABLE projects
ADD COLUMN IF NOT EXISTS business_type text DEFAULT 'unknown',
ADD COLUMN IF NOT EXISTS ai_context text DEFAULT '',
ADD COLUMN IF NOT EXISTS learned_patterns jsonb DEFAULT '[]'::jsonb,
ADD COLUMN IF NOT EXISTS success_exploits jsonb DEFAULT '[]'::jsonb,
ADD COLUMN IF NOT EXISTS total_requests_analyzed integer DEFAULT 0,
ADD COLUMN IF NOT EXISTS tokens_saved integer DEFAULT 0,
ADD COLUMN IF NOT EXISTS last_analysis timestamp with time zone DEFAULT now();
```

3. **Cr√©er chaque table individuellement** si n√©cessaire

### 3. V√©rification des tables

Apr√®s migration, v√©rifier que toutes les tables sont cr√©√©es:

```sql
SELECT table_name
FROM information_schema.tables
WHERE table_schema = 'public'
AND table_name IN (
  'request_patterns',
  'ai_memory',
  'similarity_cache',
  'compressed_requests',
  'learning_loops'
);
```

Devrait retourner 5 lignes.

### 4. Configuration des variables d'environnement

Cr√©er/modifier `.env.local`:

```env
# Supabase
NEXT_PUBLIC_SUPABASE_URL=https://[YOUR-PROJECT-ID].supabase.co
NEXT_PUBLIC_SUPABASE_ANON_KEY=[YOUR-ANON-KEY]
SUPABASE_SERVICE_KEY=[YOUR-SERVICE-KEY] # Optionnel, pour admin

# OpenAI (pour embeddings)
OPENAI_API_KEY=sk-[YOUR-KEY]

# Anthropic (pour Claude)
ANTHROPIC_API_KEY=sk-ant-[YOUR-KEY]
```

### 5. Test de connexion

Tester avec le script de v√©rification:
```bash
node scripts/apply-migrations.js
# Choisir option 3 (Verify tables only)
```

## üìä Tables cr√©√©es

| Table | Description | R√¥le |
|-------|-------------|------|
| `request_patterns` | Patterns d√©tect√©s par projet | Stocke les patterns business logic identifi√©s |
| `ai_memory` | M√©moire IA √©volutive | Contexte et apprentissages par projet |
| `similarity_cache` | Cache de similarit√© | √âconomise 95% des tokens via cache intelligent |
| `compressed_requests` | Requ√™tes compress√©es | R√©duit 5KB ‚Üí 200 bytes |
| `learning_loops` | Boucles d'apprentissage | Feedback utilisateur ‚Üí am√©lioration IA |

## üîç V√©rification post-migration

### Test rapide des fonctions
```sql
-- Test construction contexte
SELECT build_ai_context(
  (SELECT id FROM projects LIMIT 1)
);

-- Test calcul √©conomies
SELECT calculate_token_savings(
  (SELECT id FROM projects LIMIT 1)
);
```

### V√©rifier les index
```sql
SELECT indexname, tablename
FROM pg_indexes
WHERE schemaname = 'public'
AND tablename LIKE '%pattern%'
OR tablename LIKE '%memory%'
OR tablename LIKE '%cache%';
```

## ‚ö†Ô∏è Troubleshooting

### Erreur: "vector type does not exist"
```sql
CREATE EXTENSION vector;
```

### Erreur: "function gen_random_uuid() does not exist"
```sql
CREATE EXTENSION "uuid-ossp";
```

### Tables d√©j√† existantes
- Ignorer les erreurs "already exists"
- Ou supprimer et recr√©er: `DROP TABLE IF EXISTS table_name CASCADE;`

## ‚úÖ Prochaines √©tapes

1. ‚úÖ Migrations appliqu√©es
2. üìù Configurer les cl√©s API dans `.env.local`
3. üß™ Tester avec une premi√®re requ√™te
4. üìä V√©rifier le dashboard des stats

## üìö Documentation API

- **POST /api/analyze-smart**: Analyse intelligente avec cache
- **POST /api/feedback-smart**: Boucle d'apprentissage
- **GET /api/project-stats**: Statistiques par projet

Chaque projet a sa propre m√©moire isol√©e, garantissant:
- üîí Isolation compl√®te des donn√©es
- üí∞ 95% d'√©conomie sur les co√ªts API
- üß† Apprentissage continu par feedback
- ‚ö° R√©ponses instantan√©es via cache L1/L2/L3